{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments - BGP offline dataset preparation  \n",
    "Generate dataset including anomaly events for training models with the help of existing functions from project Cyberdefence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set date range for messages to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Slammer event data.\n",
    "site = 'RIPE'\n",
    "collector_ripe = 'rrc04'\n",
    "start_date, end_date = ('20030123', '20030128')\n",
    "start_date_anomaly, end_date_anomaly = ('20030125', '20030125')\n",
    "start_time_anomaly, end_time_anomaly = ('0531', '1959')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Download the message files\n",
    "A folder for each date.  \n",
    "```\n",
    "Input variables: start_date, end_date, site, collector_ripe  \n",
    "Output files: src/data_ripe/yyyymmdd/DUMP_yyyymmdd  \n",
    "```\n",
    "Original BGP message file is downloaded to folder src/data_ripe, then converted to ASCII format.  \n",
    "The output file includes BGP messages in plain text.  \n",
    "This is a message example:  \n",
    "```\n",
    "TIME: 2024-8-24 18:05:00\n",
    "TYPE: BGP4MP/BGP4MP_MESSAGE_AS4 AFI_IP\n",
    "FROM: 192.65.185.3\n",
    "TO: 192.65.185.40 \n",
    "BGP PACKET TYPE: UPDATE\n",
    "ORIGIN: IGP\n",
    "AS_PATH: 513 21320 9002 57363 57363 57363\n",
    "NEXT_HOP: 192.65.185.3\n",
    "COMMUNITIES: 20965:3 20965:4 21320:64622 21320:64698\n",
    "ANNOUNCED: 151.236.111.0/24\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataDownload import data_downloader_multi\n",
    "\n",
    "data_downloader_multi(start_date, end_date, site, collector_ripe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract features from BGP messages\n",
    "Call a C# executable to extract 37 features + 4 timestamps from the dump files.  What the extraction does is basically summarize all kind of numbers of messages by minute. For example, feature No.1 Number of annoucements means how many annoucement message there is in one minute. Feature No.5 means the average length of all the AS-path strings contained in the messages in one minute.\n",
    "```\n",
    "Input variables: start_date, end_date, site\n",
    "Input files: src/data_ripe/yyyymmdd/DUMP_yyyymmdd  \n",
    "Output files: src/data_split/DUMP_yyyymmdd_out.txt  \n",
    "```\n",
    "The following is the field definition for DUMP_yyyymmdd_out.txt:  \n",
    "```\n",
    "Columns 1-4: time (column 1: hour+minute; column 2: hour; column 3: minute; column 4: second)\n",
    "Columns 5-41: features\n",
    "\n",
    "List of features extracted from BGP update messages:\n",
    "1 Number of announcements\n",
    "2 Number of withdrawals\n",
    "3 Number of announced NLRI prefixes\n",
    "4 Number of withdrawn NLRI prefixes\n",
    "5 Average AS-path length\n",
    "6 Maximum AS-path length\n",
    "7 Average unique AS-path length\n",
    "8 Number of duplicate announcements\n",
    "9 Number of duplicate withdrawals\n",
    "10 Number of implicit withdrawals\n",
    "11 Average edit distance\n",
    "12 Maximum edit distance\n",
    "13 Inter-arrival time\n",
    "14–24 Maximum edit distance = n, n = 7, . . . , 17\n",
    "25–33 Maximum AS-path length = n, n = 7, . . . , 15\n",
    "34 Number of Interior Gateway Protocol (IGP) packets\n",
    "35 Number of Exterior Gateway Protocol (EGP) packets\n",
    "36 Number of incomplete packets\n",
    "37 Packet size (B)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.featureExtraction import feature_extractor_multi\n",
    "\n",
    "output_file_list = feature_extractor_multi(start_date, end_date, site)\n",
    "print(\"Feature extraction done for:\", output_file_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Label data points.\n",
    "Simply label every data points between the period of anomaly as 1, and 0 for others.  Output the labels to a seperate file.  \n",
    "```\n",
    "Input variables: start_date_anomaly, end_date_anomaly, start_time_anomaly, end_time_anomaly, site, output_file_list  \n",
    "Input files: src/data_split/DUMP_yyyymmdd_out.txt  \n",
    "Output files: src/STAT/labels_RIPE.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.label_generation import label_generator\n",
    "\n",
    "labels = label_generator(start_date_anomaly, end_date_anomaly, start_time_anomaly, end_time_anomaly, site, output_file_list)\n",
    "print(len(labels), \"label generated.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition data\n",
    "Merge multiple dates of data point and their labels to one dataset, then cut the dataset to training set and testing set at the portion specified.  \n",
    "The cutting rule is:  \n",
    "1. Assuming the anomaly labels in all labels are one and only one continous segment of points.\n",
    "2. Cut the anomaly label segment to two segments based on cut_pct parameter. As the result, we get the cutting point index.  \n",
    "3. Cut the dataset according to the cutting point index. The left part belongs to train set while the right part belongs to test set.  \n",
    "4. Since RNN algorithms require a sequence of data points as the input, the cutting position needs to be rounded to integral multiple of the length of the sequence (e.g. 10)\n",
    "\n",
    "\n",
    "Here is an example:  \n",
    "labels:  000001111111111000000000  \n",
    "portion: 60% train, 40% test  \n",
    "cutting point (\\*): 00000111111\\*1111000000000  \n",
    "result train labels: 00000111111  \n",
    "result test labels: 1111000000000  \n",
    "```\n",
    "Input files: src/data_split/DUMP_yyyymmdd_out.txt, src/STAT/labels_RIPE.csv  \n",
    "Input variables: labels, site, output_file_list\n",
    "Output files: src/data_split/train_64_RIPE.csv, test_64_RIPE.csv, src/STAT/train_test_stat.txt\n",
    "```\n",
    "The output file is an matrix of float values contains 37 columns of features and one label column in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_partition import data_partition\n",
    "\n",
    "cut_pct = '64' # Train: 60%, Test: 40%\n",
    "rnn_seq = 10 # 10 sequence data for RNN input\n",
    "data_partition(cut_pct, site, output_file_list, labels, rnn_seq)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Normalize data\n",
    "```\n",
    "Input variables: cut_pct, site  \n",
    "Input files: src/data_split/train_64_RIPE.csv, test_64_RIPE.csv  \n",
    "Output files: src/data_split/train_64_RIPE_n.csv, test_64_RIPE_n.csv  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data normalization done.\n"
     ]
    }
   ],
   "source": [
    "from src.data_process import normTrainTest\n",
    "\n",
    "normTrainTest(cut_pct, site)\n",
    "print(\"Data normalization done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's done. Now we can use train_64_RIPE_n.csv to train the model, and test_64_RIPE_n.csv to test the performance of the model trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
